\exercise{Optimal Control}
In this exercise, we consider a finite-horizon discrete time-varying Stochastic Linear Quadratic Regulator with Gaussian noise and time-varying quadratic reward function. Such system is defined as
%
\begin{align}
	\vec s_{t+1} = \mat A_t \vec s_t + \mat B_t a_t + \vec w_t\,, 
\end{align}
where $\vec s_t$ is the state, $a_t$ is the control signal, $\vec w_t \sim \gauss{\vec b_t}{\mat \Sigma_t}$ is Gaussian additive noise with mean $\vec b_t$ and covariance $\mat \Sigma_t$ and $t=0,1,\dots,T$ is the time horizon. 
The control signal $a_t$ is computed as
%
\begin{align}
	a_t = - \mat K_t \vec s_t + k_t
\end{align}
%
and the reward function is
%
\begin{align}
	reward_t = \begin{cases}
	- (\vec s_t - \vec r_t)\T \mat R_t (\vec s_t - \vec r_t) - a\T_t \mat H_t a_t  & \text{when \quad} t=0,1,\dots,T-1
	\\
	- (\vec s_t - \vec r_t)\T \mat R_t (\vec s_t - \vec r_t) & \text{when \quad} t=T
	\end{cases}
\end{align}
	
\textbf{Note: the notation used in Marc Toussaint's notes ``\textit{(Stochastic) Optimal Control''} is different from the one used in the lecture's slides.}	
	
\begin{questions}

%----------------------------------------------

\begin{question}{Implementation}{8}
	Implement the LQR with the following properties
	\begin{align*}
	\vec s_0 & \sim \gauss{\vec 0}{\mat I} &	T &= 50
	\\
	\mat A_t &= \begin{bmatrix}
       1 & 0.1\\
       0 & 1
    \end{bmatrix} &
	\mat B_t &= \begin{bmatrix}
       0 \\
       0.1
    \end{bmatrix}
    \\
	\vec b_t &= \begin{bmatrix}
       5\\
       0
    \end{bmatrix} & \mat \Sigma_t &= \begin{bmatrix}
       0.01 & 0\\
       0 & 0.01
    \end{bmatrix}
	\\
	\mat K_t &= \begin{bmatrix}
       5 &
       0.3
    \end{bmatrix} & 
	k_t &= 0.3 & 
	\\
	\mat H_t &= 1 & 		
	\mat R_t &= \begin{cases}
	\begin{bmatrix}
	100000 &0\\
	0&0.1
	\end{bmatrix}  & \text{if \quad $t=14$ or 40}
	\\
	\begin{bmatrix}
	0.01 &0\\
	0&0.1
	\end{bmatrix} & \text{otherwise}
	\end{cases}	
	&
	\vec r_t &= \begin{cases}
	\begin{bmatrix}
	10\\
	0
	\end{bmatrix}  & \text{if \quad} t=0,1,\ldots,14
	\\
	\begin{bmatrix}
	20\\
	0
	\end{bmatrix}  & \text{if \quad} t=15,16,\ldots,T
	\end{cases}	
	\end{align*}
	
	Execute the system 20 times.
	Plot the mean and 95\% confidence (see ``68--95--99.7 rule'' and matplotlib.pyplot.fill\_between function) over the different experiments of the state $\vec s_t$ and of the control signal $\vec a_t$ over time. 
	How does the system behave? 
	Compute and write down the mean and the standard deviation of the cumulative reward over the experiments. 
	Attach a snippet of your code.
	
\begin{answer}
	
\begin{center}
	\includegraphics[width=0.75\textwidth]{img/1a_20.png}
	\captionof{figure}{States and action of an instable controller. Mean and 95 \% confidence level.}
\end{center}

The system is instable cause the eigenvalues of the discrete system ( derived from the A-BK matrix) are: $\lambda_i = 0.985 \pm i 0.2231 i$. The eigenvalues are outside the unit circle. For this reason the system is instable and  oscillates (see state s1, s2 ).

The mean of the  cummulative reward over the experiments:  $\sum rw = -2717991.1$

The standard deviation of the cummulative reward is: 681486

\lstinputlisting[caption=Code too calc the states and actions]{code/a1_a.py}
\end{answer}

\end{question}

%---------------------------------

\begin{question}{LQR as a P controller}{4}

	The LQR can also be seen as a simple P controller of the form
	%
	\begin{align}
		a_t = \mat K_t (\vec s^\text{des}_t - \vec s_t) + k_t\,,
	\end{align}
	%
	which corresponds to the controller used in the canonical LQR system with the introduction of the target $\vec s^\text{des}_t$.
	
	Assume as target 
	\begin{align}
        \vec s^\text{des}_t = \vec r_t = \begin{cases}
        \begin{bmatrix}
        10\\
        0
        \end{bmatrix}  & \text{if \quad} t=0,1,\ldots,14
        \\
        \begin{bmatrix}
        20\\
        0
        \end{bmatrix}  & \text{if \quad} t=15,16,\ldots,T
        \end{cases}	
	\end{align}
    
    Use the same LQR system as in the previous exercise and run 20 experiments. Plot in one figure the mean and 95\% confidence (see ``68--95--99.7 rule'' and matplotlib.pyplot.fill\_between function) of the first dimension of the state, for both $\vec s^\text{des}_t = \vec r_t$ and $\vec s^\text{des}_t = \vec 0$.
\end{question}

\begin{answer}
\begin{center}
	\includegraphics[width=0.75\textwidth]{img/1b_20.png}
	\captionof{figure}{States of an instable controller. Mean and 95 \% confidence level. Once with s\_des=0 and once with s\_des=r.}
\end{center}

Since t=40 the state s1\_des=r oscillates around 20. Whereas the state s1\_des = 0 oscillates a little bit lower.
	
\end{answer}


%---------------------------------
	
	
\begin{question}{Optimal LQR}{8}
	To compute the optimal gains $\mat K_t$ and $\vec k_t$, which maximize the cumulative reward, we can use an analytic optimal solution. This controller recursively computes the optimal action by
	\begin{align}
		a_t^* &= -(\mat H_t + \mat B^T_t \mat V_{t+1}\mat B_t)^{-1}	\mat B^T_t (\mat V_{t+1} (\mat A_t \vec s_t+\mat b_t )- \vec v_{t+1} ),
	\end{align}
	which can be decomposed into
	\begin{align}
		\mat K_t &= -(\mat H_t + \mat B^T_t \mat V_{t+1}\mat B_t)^{-1}	\mat B^T_t \mat V_{t+1} \mat A_t,
		\\
		\mat k_t &= -(\mat H_t + \mat B^T_t \mat V_{t+1}\mat B_t)^{-1}	\mat B^T_t (\mat V_{t+1} \mat b_t - \vec v_{t+1}).
	\end{align}
	%
	where
	%
	\begin{align}
		\mat M_t &= \mat B_t(\mat H_t + \mat B^T_t \mat V_{t+1}\mat B_t)^{-1}	\mat B^T_t \mat V_{t+1} \mat A_t
		\\
		\mat V_t &=
		\begin{cases}
	       \mat R_t + (\mat A_t - \mat M_t)^T\mat V_{t+1}\mat A_t & \text{when \quad} t = 1...T-1
	       \\
	       \mat R_t & \text{when \quad} t = T
	    \end{cases}
	    \\
		\mat v_t &= 
		\begin{cases}
	       \mat R_t\vec r_t + (\mat A_t - \mat M_t)^T(\vec v_{t+1} - \mat V_{t+1}\mat b_t) & \text{when \quad} t = 1...T-1
	       \\
	       \mat R_t\vec r_t & \text{when \quad} t = T
	    \end{cases}
	\end{align}		 

	Run 20 experiments with $\vec s^\text{des}_t = \vec 0$ computing the optimal gains $\mat K_t$ and $\vec k_t$. Plot the mean and 95\% confidence (see ``68--95--99.7 rule'' and matplotlib.pyplot.fill\_between function) of both states for all three different controllers used so far. Use one figure per state. 
	Report the mean and std of the cumulative reward for each controller and comment the results. Attach a snippet of your code.

\begin{answer}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{img/1c_1.png}
		\captionof{figure}{Action, states and reward of the optimal controller.}
	\end{center}

	\begin{center}
		\includegraphics[width=0.75\textwidth]{img/1c_2.png}
		\captionof{figure}{State 1 of all three controllers compared.}
	\end{center}

	\begin{center}
		\includegraphics[width=0.75\textwidth]{img/1c_3.png}
		\captionof{figure}{State 2 of all three controllers compared.}
	\end{center}

	\begin{center}
		\begin{tabular}{|l|r|r|}
			\hline 
			cumulative reward & mean & std \\ 
			\hline 
			s\_des = 0 & -2739815.9 & 424072.8 \\ 
			\hline 
			s\_des = r & -105631266.4 & 11544950.4 \\ 
			\hline 
			opt & -61296.8 & 3966.4 \\ 
			\hline 
		\end{tabular}
		\captionof{table}{Rewards of the diffeerent controller}
	\end{center}	

	The only important part of the reward is the first state at t=14 and t=40. There R equals 100000 and because of this these timesteps have a much higher weight in the cumulative reward. All other timesteps have a R smaller 1 and a really small weight. State 2 has a small R at every timestep, so it is not important for the cumulative reward. 
	
	The desired state value is described by r. It equals 10 at t=14 and 20 at t=40. So the trajectory should go through this points (marked with red dots) to minimize the reward.

	The optimal controller meats this requirement, like you can see in the plot. The first controller is unstable and oscillates, but its trajectory also is near to the desired points. The second trajectory is also unstable and it is far away from the desired points. Because of this, the second trajectory has a really high negative reward. The first controller has a much smaller negative reward, but it also is very high. The optimal controller has a really small negative reward, compared too the two other controllers.
	\lstinputlisting[caption=code too calc the optimal controller]{code/a1_c.py}
\end{answer}
\end{question}

%----------------------------------------------

\end{questions}
