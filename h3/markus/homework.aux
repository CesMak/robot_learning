\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Problem~3.1}Optimal Control [20{} Points\relax ]}{1}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces States and action of an instable controller. Mean and 95 \% confidence level.\relax }}{2}{figure.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Code too calc the states and actions}{2}{lstlisting.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces States of an instable controller. Mean and 95 \% confidence level. Once with s\_des=0 and once with s\_des=r.\relax }}{4}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Action, states and reward of the optimal controller.\relax }}{5}{figure.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces State 1 of all three controllers compared.\relax }}{6}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces State 2 of all three controllers compared.\relax }}{6}{figure.10}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Rewards of the diffeerent controller\relax }}{6}{table.2}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}code too calc the optimal controller}{7}{lstlisting.4}}
\newlabel{countitems@0}{{20}{8}{Optimal Control [\nextitemizecount {} Points\ifnum \bonusnextitemizecount =0\else \ + \bonusnextitemizecount {} Bonus \fi ]}{lstnumber.4.88}{}}
\newlabel{bonuscountitems@0}{{0}{8}{Optimal Control [\nextitemizecount {} Points\ifnum \bonusnextitemizecount =0\else \ + \bonusnextitemizecount {} Bonus \fi ]}{lstnumber.4.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Problem~3.2}Reinforcement Learning [34{} Points\ + 8{} Bonus ]}{9}{subsection.3.2}}
\newlabel{Eq:J}{{12}{10}{Reinforcement Learning [\nextitemizecount {} Points\ifnum \bonusnextitemizecount =0\else \ + \bonusnextitemizecount {} Bonus \fi ]}{equation.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces T=15, finite horizon Value Function\relax }}{10}{figure.14}}
\newlabel{fig:2a1}{{14}{10}{T=15, finite horizon Value Function\relax }{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces T=15, finite horizon Policy\relax }}{10}{figure.15}}
\newlabel{fig:2a2}{{15}{10}{T=15, finite horizon Policy\relax }{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces T=10, finite horizon Policy\relax }}{10}{figure.16}}
\newlabel{fig:2a3}{{16}{10}{T=10, finite horizon Policy\relax }{figure.16}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}code of the value iteration algorithm}{11}{lstlisting.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces T=100, gamma=0.8 infinite horizon Value Function\relax }}{13}{figure.19}}
\newlabel{fig:2c1}{{19}{13}{T=100, gamma=0.8 infinite horizon Value Function\relax }{figure.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces T=100, gamma=0.8, infinite horizon Policy\relax }}{13}{figure.20}}
\newlabel{fig:2c2}{{20}{13}{T=100, gamma=0.8, infinite horizon Policy\relax }{figure.20}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {8}code of the value iteration algorithm with converge}{13}{lstlisting.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces T=15, finite horizon Value Function\relax }}{15}{figure.23}}
\newlabel{fig:2d1}{{23}{15}{T=15, finite horizon Value Function\relax }{figure.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces T=15, finite horizon Policy\relax }}{15}{figure.24}}
\newlabel{fig:2d2}{{24}{15}{T=15, finite horizon Policy\relax }{figure.24}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {10}code of the value iteration algorithm with probabilistic model}{15}{lstlisting.10}}
\expandafter\ifx\csname c@totalPoints@totc\endcsname\relax\newcounter{totalPoints@totc}\fi\setcounter{totalPoints@totc}{54}
\expandafter\ifx\csname c@bonusPoints@totc\endcsname\relax\newcounter{bonusPoints@totc}\fi\setcounter{bonusPoints@totc}{8}
\newlabel{countitems@1}{{34}{17}{Reinforcement Learning [\nextitemizecount {} Points\ifnum \bonusnextitemizecount =0\else \ + \bonusnextitemizecount {} Bonus \fi ]}{equation.3.21}{}}
\newlabel{bonuscountitems@1}{{8}{17}{Reinforcement Learning [\nextitemizecount {} Points\ifnum \bonusnextitemizecount =0\else \ + \bonusnextitemizecount {} Bonus \fi ]}{equation.3.21}{}}
